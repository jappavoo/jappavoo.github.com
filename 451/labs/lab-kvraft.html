<!DOCTYPE html>
<html>
	<head>
		<link rel="stylesheet" href="../style.css" type="text/css">
		<title>CS451/651 Lab 3: Fault-tolerant Key/Value Service</title>
	</head>
	<body>
		<div align="center">
			<h2><a href="../index.html">CS451/651</a></h2>
			<h1>CS451/651 Lab 3: Fault-tolerant Key/Value Service</h1>
			<h3>Due Part A: Nov 1 11:59pm</h3>
			<h3>Due Part B: Nov 15 11:59pm</h3>
		</div>
		<hr>

		<h3>Introduction</h3>

		<p>
		In this lab you will build a fault-tolerant key-value storage
		service using your Raft library from
		<a href="lab-raft.html">lab 2</a>. You will build your
		key-value service as a replicated state machine, consisting of
		several key-value servers that coordinate their activities
		through the Raft log. Your key/value service should continue to
		process client requests as long as a majority of the servers
		are alive and can communicate, in spite of other failures or
		network partitions.
  
		<p>
		Your system will consist of clients and key/value servers,
		where each key/value server also acts as a Raft peer. Clients
		send <tt>Put()</tt>, <tt>Append()</tt>, and <tt>Get()</tt> RPCs
		to key/value servers (called kvraft servers), who then place
		those calls into the Raft log and executes them in order. A
		client can send an RPC to any of the kvraft servers, but should
		retry by sending to a different server if the server is not
		currently a Raft leader, or if there's a failure. If the
		operation is committed to the Raft log (and hence applied to
		the key/value state machine), its result is reported to the
		client. If the operation failed to commit (for example, if the
		leader was replaced), the server reports an error, and the
		client retries with a different server.

		<p>
		This lab has two parts. In part A, you will implement the
		service without worrying that the Raft log can grow without
		bound. In part B, you will implement snapshots (Section 7 in
		the paper), which will allow Raft to garbage collect old log
		entries.

		<ul class="hints">
			<li>
			This lab doesn't require you to write much code, but
			you will most likely spend a substantial amount of time
			thinking and staring at debugging logs to figure out
			why your implementation doesn't work. Debugging will be
			more challenging than in the Raft lab because there are
			more components that work asynchronously of each other.
			Start early.

			<li>
			You should reread the
			<a href="../papers/raft-extended.pdf">extended Raft paper</a>,
			in particular Sections 7 and 8. For a wider
			perspective, have a look at Chubby, Raft Made Live,
			Spanner, Zookeeper, Harp, Viewstamped Replication, and
			<a href="http://static.usenix.org/event/nsdi11/tech/full_papers/Bolosky.pdf">Bolosky et al.</a>
		</ul>

		<h3>Collaboration Policy</h3>

		You must write all the code you hand in for CS451/651, except for
		code that we give you as part of the assignment. You are not
		allowed to look at anyone else's solution, you are not allowed
		to look at code from previous years, and you are not allowed to
		look at other Raft implementations. You may discuss the
		assignments with other students, but you may not look at or
		copy each others' code. Please do not publish your code or make
		it available to future CS451/651 students -- for example, please do
		not make your code visible on GitHub (instead, create a private
		repository on MIT's <a href="https://github.mit.edu/">GitHub
		deployment</a>).

		<h3>Getting Started</h3>

		<p>
		Do a <tt>git pull</tt> to get the latest lab software. We
		supply you with new skeleton code and new tests in
		<tt>src/kvraft</tt>. You will need to modify
		<tt>kvraft/client.go</tt>, <tt>kvraft/server.go</tt>, and
		perhaps <tt>kvraft/common.go</tt>.

		<p>
		To get up and running, execute the following commands:
		<pre>
$ export GOROOT=/research/sesa/451/go
$ export PATH=$PATH:$GOROOT/bin
$ export GOPATH=$HOME/451
$ cd $GOPATH
$ git pull
...
$ cd src/kvraft
$ go test
...
$</pre>

		When you're done, your implementation should pass all the tests
		in the <tt>src/kvraft</tt> directory:

		<pre>
$ go test
Test: One client ...
  ... Passed
Test: concurrent clients ...
  ... Passed
Test: unreliable ...
  ... Passed
...
PASS
ok  	kvraft	345.032s</pre>


		<h3>Part A: Key/value service without log compaction</h3>
		<p>
		The service supports three RPCs: <tt>Put(key, value)</tt>,
		<tt>Append(key, arg)</tt>, and <tt>Get(key)</tt>. It maintains
		a simple database of key/value pairs. <tt>Put()</tt> replaces
		the value for a particular key in the database, <tt>Append(key,
		arg)</tt> appends arg to key's value, and <tt>Get()</tt>
		fetches the current value for a key. An <tt>Append</tt> to
                a non-existant key should act like <tt>Put</tt>.

		<p>
		You will implement the service as a replicated state machine
		consisting of several kvservers. Your kvraft client code
		(<tt>Clerk</tt> in <tt>src/kvraft/client.go</tt>) should try
		different kvservers it knows about until one responds
		positively. As long as a client can contact a kvraft server
		that is a Raft leader in a majority partition, its operations
		should eventually succeed.

		<div class="todo">
			<p>
			Your first task is to implement a solution that works
			when there are no dropped messages, and no failed
			servers. Note that your service must provide
			<em>sequential consistency</em> to applications that
			use its client interface. That is, completed
			application calls to the <tt>Clerk.Get()</tt>,
			<tt>Clerk.Put()</tt>, and <tt>Clerk.Append()</tt>
			methods in <tt>kvraft/client.go</tt> must appear to
			have affected all kvservers in the same order, and have
			at-most-once semantics. A <tt>Clerk.Get(key)</tt>
			should see the value written by the most recent
			<tt>Clerk.Put(key, &hellip;)</tt> or
			<tt>Clerk.Append(key, &hellip;)</tt> (in the total
			order).

			<p>
			A reasonable plan of attack may be to first fill in the
			<tt>Op</tt> struct in <tt>server.go</tt> with the
			"value" information that kvraft will use Raft to agree
			on (remember that <tt>Op</tt>
			field names must start with capital letters, since they
			will be sent through RPC), and then implement the
			<tt>PutAppend()</tt> and <tt>Get()</tt> handlers in
			<tt>server.go</tt>. The handlers should enter an
			<tt>Op</tt> in the Raft log using <tt>Start()</tt>, and
			should reply to the client when that log entry is committed.
			Note that you <strong>cannot</strong> execute
			an operation until the point at which it is committed in
			the log (i.e., when it arrives on the Raft
			<tt>applyCh</tt>).

			<p>
			You have completed this task when you
			<strong>reliably</strong> pass the first test in the
			test suite: "One client". You may also find that you
			can pass the "concurrent clients" test, depending on
			how sophisticated your implementation is.
		</div>

		<p class="note">
		Your kvraft servers should not directly communicate; they
		should only interact with each other through the Raft log.

		<ul class="hints">
			<li>
			After calling <tt>Start()</tt>, your kvraft
			servers will need to wait for Raft to complete
			agreement. Commands that have been agreed upon arrive
			on the <tt>applyCh</tt>. You should think carefully
			about how to arrange your code so that your code will
			keep reading <tt>applyCh</tt>, while
			<tt>PutAppend()</tt> and <tt>Get()</tt> handlers submit
			commands to the Raft log using <tt>Start()</tt>. It is
			easy to achieve deadlock between the kvserver and its
			Raft library.

                        <li>
                        Your solution needs to handle the case in
                        which a leader has called Start() for a client
                        RPC, but loses its leadership before the
                        request is committed to the log. In this case
                        you should arrange for the client to re-send
                        the request to other servers until it finds
                        the new leader. One way to do this is for the
                        server to detect that it has lost leadership,
                        by noticing that a different request has
                        appeared at the index returned by Start(), or
                        that the term reported by Raft.GetState() has
                        changed. If the ex-leader is partitioned by
                        itself, it won't know about new leaders; but
                        any client in the same partition won't be able
                        to talk to a new leader either, so it's OK in
                        this case for the server and client to wait
                        indefinitely until the partition heals.

                        <li>
                        You will probably have to modify your client
                        Clerk to remember which server turned out to
                        be the leader for the last RPC, and send the
                        next RPC to that server first. This will avoid
                        wasting time searching for the leader on every
                        RPC, which may help you pass some of the tests
                        quickly enough.

			<li>
			A kvraft server should not complete a <tt>Get()</tt>
			RPC if it is not part of a majority (so that it does
			not serve stale data). A simple solution is to enter
			every <tt>Get()</tt> (as well as each <tt>Put()</tt>
			and <tt>Append()</tt>) in the Raft log. You don't have
			to implement the optimization for read-only operations
			that is described in Section 8.
		</ul>

		In the face of unreliable connections and node failures, your
		clients may send RPCs multiple times until it finds a kvraft
		server that replies positively. One consequence of this is that
		you must ensure that each application call to
		<tt>Clerk.Put()</tt> or <tt>Clerk.Append()</tt> must appear in
		that order just once (i.e., write the key/value database just
		once).

		<p class="todo">
		Add code to cope with duplicate client requests, including
		situations where the client sends a request to a kvraft leader
		in one term, times out waiting for a reply, and re-sends the
		request to a new leader in another term. The client request
		should always execute just once. To pass part A, your service
		should <strong>reliably</strong> pass all tests through
		<tt>TestPersistPartitionUnreliable()</tt>.

		<ul class="hints">
			<li>
			You will need to uniquely identify client operations to
			ensure that they execute just once. You can assume that
			each clerk has only one outstanding <tt>Put</tt>,
			<tt>Get</tt>, or <tt>Append</tt>.

			<li>
			You must make sure that your scheme for duplicate
			detection frees server memory quickly, for example by
			having the client tell the servers which RPCs it has
			heard a reply for. It's OK to piggyback this
			information on the next client request.
		</ul>

		<h3>Part B: Key/value service with log compaction</h3>

		<p>
		In order to allow Raft to discard old log entries so that the
		log doesn't grow without bounds, you will implement snapshots
		as described in Section 7 of
		<a href="../papers/raft-extended.pdf">extended Raft paper</a>.
		Section 7 provides only an outline; you will have to figure out
		the details.

		<p>
		You should spend some time figuring out what the interface will
		be between your Raft library and your service so that your Raft
		library can discard log entries. Think about how your Raft will
		operate while storing only the tail of the log, and how it will
		discard old log entries. You should discard them in a way that
		causes the underlying memory to be free (so that the Go garbage
		collector can re-use the memory).

		<p>
		The kvraft tester passes <tt>maxraftstate</tt> to your
		<tt>StartKVServer()</tt>. <tt>maxraftstate</tt> indicates the
		maximum allowed size of your persistent Raft state in bytes (including
		the log, but not including snapshots). Whenever your key/value
		server detects that the Raft state size is approaching this
		threshold, it should save a snapshot, and tell the Raft library
                that it has snapshotted, so that Raft can discard old
                log entries.

		<p class="todo">
		Modify your Raft library (in <tt>src/raft/raft.go</tt>) so that
                it can discard old log entries and still operate with only
                the tail of the log.
		Make sure you pass all the Raft
		tests after making these changes.

		<p class="todo">
		Modify your kvraft server so that it detects when the persisted
                Raft state grows too large, and then saves a snapshot and tells
                Raft that it can discard old log entries.
                Save each snapshot with persister.SaveSnapshot()
                (don't use files).

                <p class="todo">
                Modify your Raft leader code to send an InstallSnapshot
                RPC to a follower when the leader has discarded the log
                entries the follower needs.
		When a follower receives an InstallSnapshot RPC,
                your Raft code will need to send the included snapshot to
		its kvraft. You can use the <tt>applyCh</tt>
		for this purpose &mdash; see the <tt>UseSnapshot</tt>
		field. Your
		solution is complete when you pass the remaining tests
		<strong>reliably</strong>.

		<p class="note">
		The <tt>maxraftstate</tt> limit applies to the GOB-encoded
		bytes your Raft passes to <tt>persister.SaveRaftState()</tt>.

		<ul class="hints">
			<li>
			Think about when a kvserver should snapshot its state
			and what should be included in the snapshot. You must
			store each snapshot in the persister object using
			<tt>SaveSnapshot()</tt>,
			<strong>not</strong> along with the other Raft state in
			<tt>SaveRaftState()</tt>. You can read the
			latest stored snapshot using <tt>ReadSnapshot()</tt>.

			<li>
			Remember that your kvserver must be able to detect
			duplicate client requests across checkpoints, so any
			state you are using to detect them must be included in
			the snapshots.

			<li>
			Make sure you pass <tt>TestSnapshotRPC</tt> before
			moving on to the other Snapshot tests.
		</ul>

		<h3>Handin procedure</h3>

		<div class="important">
			<p>
			Before submitting, please run <em>all</em> the tests
      one final time. <b>You</b> are responsible for making sure your code
      works. Keep in mind that the more obscure corner cases may not appear on
      every run, so it's a good idea to run the tests multiple times.

			<pre>$ go test</pre>
		</div>

		<p>
Submit your code via the gsubmit command using the provided makefile

Depending on which part you are submitting choose the right make target
for the example below
<pre>
$ cd ~/451
$ make lab3a|lab3b
</pre>

You can use the gsubmit command to check that it was submitted correctly.


		<p class="note">
		You may submit multiple times. We will use the timestamp of
		your <strong>last</strong> submission for the purpose of
		calculating late days. Your grade is determined by the score
		your solution <strong>reliably</strong> achieves when we run
		the tester on our test machines.

		<hr>

		<address>
			Please post questions on <a href="http://piazza.com">Piazza</a>.
		</address>

	</body>
</html>
<!--  LocalWords:  RPCs viewservice src pbservice cd view's PingInterval ack pb
 -->
<!--  LocalWords:  DeadPings Viewservice ViewServer PingArgs Viewnum viewservice
 -->
<!--  LocalWords:  Handin gzipped czvf whoami tgz GOPATH TestBasicFail GetReply
 -->
<!--  LocalWords:  TestFailPut TestConcurrentSame TestPartition PutReply Raft
 -->
<!--  LocalWords:  kvraft instance's Viewstamped al paxos TestBasic seq Go's
 -->
<!--  LocalWords:  ndecided TestForget px int bool ok Min Raft's piggyback un
 -->
<!--  LocalWords:  struct Op IDs pm structs marshall unmarshall class's
 -->
<!--  LocalWords:  StartServer kvrafts website API
 -->
