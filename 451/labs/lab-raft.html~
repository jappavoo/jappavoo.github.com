<!DOCTYPE html>
<html>
	<head>
		<link rel="stylesheet" href="../style.css" type="text/css">
		<title>6.824 Lab 2: Raft</title>
	</head>
	<body>
		<div align="center">
			<h2><a href="../index.html">6.824</a> - Spring 2016</h2>
			<h1>6.824 Lab 2: Raft</h1>
			<h3>Due: Fri Feb 26, 11:59pm</h3>
		</div>
		<hr>

		<h3>Introduction</h3>

		<p>
		This is the first in a series of labs in which you'll build a
		fault-tolerant key/value storage system. You'll start in this
		lab by implementing Raft, a replicated state machine protocol.
		In the next lab lab you'll build a key/value service on top of
		Raft. Then you will &ldquo;shard&rdquo; your service for higher
		performance, and finally implement transactional operations
		across shards.

		<p>
		A replicated service (e.g., key/value database) uses Raft to
		help manage its replica servers. The point of having replicas
		is so that the service can continue operating even if some of
		the replicas experience failures (crashes or a broken or flaky
		network). The challenge is that, due to these failures, the
		replicas won't always hold identical data; Raft helps the
		service sort out what the correct data is.

		<p>
		Raft's basic approach is to implement a replicated state
		machine. Raft organizes client requests into a sequence, called
		the log, and ensures that all the replicas agree on the the
		contents of the log. Each replica executes the client requests
		in the log in the order they appear in the log, applying those
		requests to the service's state. Since all the live replicas
		see the same log contents, they all execute the same requests
		in the same order, and thus continue to have identical service
		state. If a server fails but later recovers, Raft takes care of
		bringing its log up to date. Raft will continue to operate as
		long as at least a majority of the servers are alive and can
		talk to each other. If there is no such majority, Raft will
		make no progress, but will pick up where it left off as soon as
		a majority is alive again.

		<p>
		In this lab you'll implement Raft in the form of Go object type
		with associated methods, meant to be used as a module in a
		larger service. A set of Raft instances talk to each other with
		RPC to maintain replicated logs. Your Raft interface will
		support an indefinite sequence of numbered commands, also
		called log entries. The entries are numbered with <em>index
		numbers</em>. The log entry with a given index will eventually
		be committed. At that point, your Raft should send the log
		entry to the larger service for it to execute.

		<p class="note">
		Only RPC may be used for interaction between different Raft
		instances. For example, different instances of your Raft
		implementation are not allowed to share Go variables.
                Your implementation should not use files at all.

		<p>
		In this lab you'll implement most of the Raft design
		described in the extended paper, including saving
		persistent state and reading it after a node fails and
		then restarts. You will not implement cluster
		membership changes (Section 6) or log compaction /
		snapshotting (Section 7).


		<p>
		You should consult the
		<a href="../papers/raft-extended.pdf">extended Raft paper</a>
		and the Raft lecture notes. You may also find this
		<a href="http://thesecretlivesofdata.com/raft/">illustrated Raft guide</a>
		useful to get a sense of the high-level workings of Raft. For a
		wider perspective, have a look at Paxos, Chubby, Paxos Made
		Live, Spanner, Zookeeper, Harp, Viewstamped Replication, and
		<a href="http://static.usenix.org/event/nsdi11/tech/full_papers/Bolosky.pdf">Bolosky et al.</a>

		<ul class="hints">
			<li>
			Start early. Although the amount of code to implement
			isn't large, getting it to work correctly will be very
			challenging. Both the algorithm and the code is tricky
			and there are many corner cases to consider. When one
			of the tests fails, it may take a bit of puzzling to
			understand in what scenario your solution isn't
			correct, and how to fix your solution.

			<li>
			Read and understand the
			<a href="../papers/raft-extended.pdf">extended Raft paper</a>
			and the Raft lecture notes before you start. Your
      implementation should follow the paper's description
      closely, since that's what the tests expect. Figure 2 may
      be useful as a pseudocode reference.
		</ul>

		<h3>Collaboration Policy</h3>

		You must write all the code you hand in for 6.824, except for
		code that we give you as part of the assignment. You are not
		allowed to look at anyone else's solution, you are not allowed
		to look at code from previous years, and you are not allowed to
		look at other Raft implementations. You may discuss the
		assignments with other students, but you may not look at or
		copy each others' code. Please do not publish your code or make
		it available to future 6.824 students -- for example, please do
		not make your code visible on GitHub (instead, create a private
		repository on MIT's <a href="https://github.mit.edu/">GitHub
		deployment</a>).

		<h3>Getting Started</h3>

		<p>
		Do a <tt>git pull</tt> to get the latest lab software. We
		supply you with skeleton code and tests in <tt>src/raft</tt>,
		and a simple RPC-like system in <tt>src/labrpc</tt>.

		<p>
		To get up and running, execute the following commands:
		<pre>
$ setup ggo_v1.5
$ cd ~/6.824
$ git pull
...
$ cd src/raft
$ GOPATH=~/6.824
$ export GOPATH
$ go test
Test: initial election ...
--- FAIL: TestInitialElection (5.03s)
	config.go:270: expected one leader, got 0
Test: election after network failure ...
--- FAIL: TestReElection (5.03s)
	config.go:270: expected one leader, got 0
...
$</pre>

		When you're done, your implementation should pass all the tests
		in the <tt>src/raft</tt> directory:

		<pre>
$ go test
Test: initial election ...
  ... Passed
Test: election after network failure ...
  ... Passed
...
PASS
ok  	raft	162.413s</pre>

		<h3>Your Job</h3>

		You should implement Raft by adding code to
		<tt>raft/raft.go</tt>. In that file you'll find a bit of
		skeleton code, plus some examples of how to send and receive
		RPCs, and examples of how to save and restore persistent state.

		<p>
		Your implementation must support the following interface, which
		the tester and (eventually) your key/value server will use.
		You'll find more details in comments in <tt>raft.go</tt>.

		<pre>
// create a new Raft server instance:
rf := Make(peers, me, persister, applyCh)

// start agreement on a new log entry:
rf.Start(command interface{}) (index, term, isleader)

// ask a Raft for its current term, and whether it thinks it is leader
rf.GetState() (term, isLeader)

// each time a new entry is committed to the log, each Raft peer
// should send an ApplyMsg to the service (or tester).
type ApplyMsg</pre>

		<p>
		A service calls <tt>Make(peers,me,&hellip;)</tt> to create a
		Raft peer. The peers argument is an array of established RPC
		connections, one to each Raft peer (including this one). The
		<tt>me</tt> argument is the index of this peer in the peers
		array. <tt>Start(command)</tt> asks Raft to start the processing
		to append the command to the replicated log. <tt>Start()</tt>
		should return immediately, without waiting for for this process
		to complete. The service expects your implementation to send an
		<tt>ApplyMsg</tt> for each new committed log entry to the
		<tt>applyCh</tt> argument to <tt>Make()</tt>.

		<p>
		Your Raft peers should exchange RPCs using the labrpc Go
		package that we provide to you. It is modeled after Go's
		<a href="https://golang.org/pkg/net/rpc/">rpc library</a>, but
		internally uses Go channels rather than sockets.
		<tt>raft.go</tt> contains some example code that sends an RPC
		(<tt>sendRequestVote()</tt>) and that handles an incoming RPC
		(<tt>RequestVote()</tt>).

		<p class="todo">
		Implement leader election and heartbeats (empty
		<tt>AppendEntries</tt> calls). This should be sufficient for a
		single leader to be elected, and to stay the leader, in the
		absence of failures. Once you have this working, you should be
		able to pass the first two "go test" tests.

		<ul class="hints">
			<li>
			Add any state you need to keep to the <tt>Raft</tt>
			struct in <tt>raft.go</tt>. Figure 2 in the paper may
			provide a good guideline. You'll also need to define a
			struct to hold information about each log entry.
			Remember that the field names any structures you will
			be sending over RPC must start with capital letters, as
			must the field names in any structure passed inside an
			RPC.

			<li>
			You should start by implementing Raft leader election.
			Fill in the <tt>RequestVoteArgs</tt> and
			<tt>RequestVoteReply</tt> structs, and modify
			<tt>Make()</tt> to create a background goroutine that
			starts an election (by sending out <tt>RequestVote</tt>
			RPCs) when it hasn't heard from another peer for a
			while. For election to work, you will also need to
			implement the <tt>RequestVote()</tt> RPC handler so
			that servers will vote for one another.

			<li>
			To implement heartbeats, you will need to define an
			<tt>AppendEntries</tt> RPC structs (though you may not
			need all the arguments yet), and have the leader send
			them out periodically. You will also have to write an
			<tt>AppendEntries</tt> RPC handler method that resets
			the election timeout so that other servers don't step
			forward as leaders when one has already been elected.

			<li>
			Make sure the timers in different Raft peers are not
			synchronized.  In particular, make sure the election
			timeouts don't always fire at the same time, or else
			all peers will vote for themselves and no one will
			become leader.
		</ul>

		While being able to elect a leader is useful, we want to use
		Raft to keep a consistent, replicated log of operations. To do
		so, we need to have the servers accept client operations
		through <tt>Start()</tt>, and insert them into the log. In
		Raft, only the leader is allowed to append to the log, and
		should disseminate new entries to other servers by including
		them in its outgoing <tt>AppendEntries</tt> RPCs.

		<p class="todo">
		Implement the leader and follower code to append new log entries.
		This will involve implementing <tt>Start()</tt>, completing the
		<tt>AppendEntries</tt> RPC structs, sending them, and fleshing
		out the <tt>AppendEntry</tt> RPC handler. Your goal should
		first be to pass the <tt>TestBasicAgree()</tt> test (in
		<tt>test_test.go</tt>). Once you have that working, you should
		try to get all the tests before the "basic persistence" test to
		pass.

		<ul class="hints">
			<li>
			A significant portion of this lab will consist of
			making your implementation robust against various kinds
			of failures. You will need to implement the election
			restriction (section 5.4.1 in the paper). The next set
			of tests have to do with various failure cases, in
			which some servers don't receive some RPCs and some
			servers occasionally crash and restart.

			<li>
			While the Raft leader is the only server that causes
			entries to be appended to the log, all the servers need
			to independently give newly committed entries to their local service
			replica (via their own <tt>applyCh</tt>). Because of this, you
			should try to keep these two activities as separate as
			possible.

			<li>
			Figure out the minimum number of messages Raft should
			use when reaching agreement in non-failure cases and
			make your implementation use that minimum.
		</ul>

		<p>
		A Raft-based server must be able to pick up where it left off,
		and continue if the computer it's on reboots. This requires
		that Raft keep persistent state that survives a reboot (the
		paper's Figure 2 mentions which state should be persistent).

		<p>
		A &ldquo;real&rdquo; implementation would do this by writing
		Raft's persistent state to disk each time it changes, and reading the latest saved
                state from
		disk when restarting after a reboot. Your implementation won't use
		the disk; instead, it will save and restore persistent state
		from a <tt>Persister</tt> object (see <tt>persister.go</tt>).
		Whoever calls <tt>Make()</tt> supplies a <tt>Persister</tt>
		that initially holds Raft's most recently persisted state (if
		any). Raft should initialize its state from that
		<tt>Persister</tt>, and should use it to save its persistent
		state each time the state changes. You can use the
		<tt>ReadRaftState()</tt> and <tt>SaveRaftState()</tt> methods
		for this respectively.

		<p class="todo">
		Implement persistence by first adding code to serialize any
		state that needs persisting in <tt>persist()</tt>, and to
		unserialize that same state in <tt>readPersist()</tt>. You now
		need to determine at what points in the Raft protocol your
		servers are required to persist their state, and insert calls
		to <tt>persist()</tt> in those places. Once this code is
		complete, you should pass the remaining tests.  You may want to
		first try and pass the "basic persistence" test (<tt>go test
		-run 'TestPersist1$'</tt>), and then tackle the remaining ones.

		<p class="note">
		You will need to encode the state as an array of bytes in order
		to pass it to the <tt>Persister</tt>; <tt>raft.go</tt> contains
		some example code for this in <tt>persist()</tt> and
		<tt>readPersist()</tt>.

		<p class="note">
		In order to avoid running out of memory, Raft must periodically
		discard old log entries, but you <strong>do not</strong> have
		to worry about garbage collecting the log in this lab. You will
		implement that in the next lab by using snapshotting (Section 7
		in the paper).

		<ul class="hints">
			<li>
			Similarly to how the RPC system only sends structure
			field names that begin with upper-case letters, and
			silently ignores fields whose names start with
			lower-case letters, the GOB encoder you'll use to save
			persistent state only saves fields whose names start
			with upper case letters. This is a common source of
			mysterious bugs, since Go doesn't warn you.


			<li>
      In order to pass some of the challenging tests towards the end, such as
      those marked "unreliable", you will need to implement the optimization to
      allow a follower to back up the leader's nextIndex by more than one entry
      at a time. See the description in the <a
      +href="../papers/raft-extended.pdf">extended Raft paper</a> starting at
    the bottom of page 7 and top of page 8 (marked by a gray line).
		</ul>

		<h3>Handin procedure</h3>

		<div class="important">
			<p>
			Before submitting, please run <em>all</em> the tests
      one final time. <b>You</b> are responsible for making sure your code
      works. Keep in mind that the more obscure corner cases may not appear on
      every run, so it's a good idea to run the tests multiple times.

			<pre>$ go test</pre>
		</div>

		<p>
		Submit your code via the class's submission website, located at
		<a href="https://6824.scripts.mit.edu:444/submit/handin.py/">https://6824.scripts.mit.edu:444/submit/handin.py/</a>.

		<p>
		You may use your MIT Certificate or request an API key via
		email to log in for the first time. Your API key (<tt>XXX</tt>)
		is displayed once you logged in, which can be used to upload
		the lab from the console as follows.

		<pre>
$ cd "$GOPATH"
$ echo "XXX" &gt; api.key
$ make lab2</pre>

		<p class="important">
		Check the submission website to make sure you submitted a working lab!

		<p class="note">
		You may submit multiple times. We will use the timestamp of
		your <strong>last</strong> submission for the purpose of
		calculating late days. Your grade is determined by the score
		your solution <strong>reliably</strong> achieves when we run
		the tester on our test machines.

		<hr>

		<address>
			Please post questions on <a href="http://piazza.com">Piazza</a>.
		</address>

	</body>
</html>
<!--  LocalWords:  transactional RPC snapshotting Paxos Viewstamped -->
<!--  LocalWords:  Bolosky et al else's github src labrpc cd ok RPCs -->
<!--  LocalWords:  TestInitialElection TestReElection rf persister -->
<!--  LocalWords:  applyCh isleader GetState isLeader rpc -->
<!--  LocalWords:  ApplyMsg Persister's SaveRaftState ReadRaftState -->
<!--  LocalWords:  readPersist sendRequestVote RequestVote struct -->
<!--  LocalWords:  RequestVoteArgs RequestVoteReply structs goroutine -->
<!--  LocalWords:  AppendEntries AppendEntry TestBasicAgree -->
<!--  LocalWords:  InstallSnapshot -->
